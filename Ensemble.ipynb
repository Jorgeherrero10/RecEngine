{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xt/zct85k1107sfr31qsfny__zm0000gn/T/ipykernel_48849/3246529824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVDpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSlopeOne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNBasic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVDpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNBasic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNWithMeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNNWithZScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "#!pip install cupy-cuda12x \n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from surprise import Reader, Dataset, SVD, SVDpp, NMF, accuracy, Prediction, AlgoBase, SlopeOne, KNNBasic\n",
    "from surprise.model_selection import train_test_split, cross_validate, KFold\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from surprise import SVD, SVDpp, NMF, KNNBasic, KNNWithMeans, KNNWithZScore\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import warnings  # Added this line\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "my_seed = 1234\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "# Load the ratings data\n",
    "ratings_df = pd.read_csv('data/rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove -1 values (implicit feedback) to focus on explicit feedback only\n",
    "ratings_df = ratings_df[ratings_df['rating'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick 5% of users\n",
    "all_users = ratings_df['user_id'].unique()\n",
    "sample_size = int(0.05 * len(all_users))\n",
    "sampled_users = np.random.choice(all_users, size=sample_size, replace=False)\n",
    "\n",
    "# Filter ratings to keep only the sampled users\n",
    "ratings_sampled = ratings_df[ratings_df['user_id'].isin(sampled_users)]\n",
    "\n",
    "# Further sample data for efficiency if needed\n",
    "ratings_filtered_sample = ratings_sampled.sample(n=35000, random_state=my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scale of ratings\n",
    "min_rating = ratings_sampled['rating'].min()\n",
    "max_rating = ratings_sampled['rating'].max()\n",
    "\n",
    "# Create a Surprise Reader object\n",
    "reader = Reader(rating_scale=(min_rating, max_rating))\n",
    "\n",
    "# Load the data into Surprise format\n",
    "anime_data = Dataset.load_from_df(ratings_sampled[['user_id', 'anime_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "trainset, testset = train_test_split(anime_data, test_size=.20, random_state=my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Mean Baseline\n",
    "class GlobalMeanBaseline(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        return self.trainset.global_mean\n",
    "    \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        return self\n",
    "\n",
    "# Popular Items Baseline (already implemented in the RecSys project)\n",
    "class PopularBaseline(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        # Convert inner item id to raw item id\n",
    "        try:\n",
    "            raw_iid = self.trainset.to_raw_iid(i)\n",
    "        except ValueError:\n",
    "            # Item not in trainset, return global mean\n",
    "            return self.trainset.global_mean\n",
    "\n",
    "        if raw_iid in self.mean_rating_per_item_df.index:\n",
    "            return self.mean_rating_per_item_df.loc[raw_iid]['rating']\n",
    "        else:\n",
    "            # Item was in trainset but somehow not in mean ratings (shouldn't happen with current fit)\n",
    "            return self.trainset.global_mean\n",
    "    \n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        \n",
    "        # Use raw ids directly from all_ratings()\n",
    "        ratings_df = pd.DataFrame([[self.trainset.to_raw_iid(i), r] \n",
    "                                   for (_, i, r) in self.trainset.all_ratings()],\n",
    "                                  columns=['item', 'rating'])\n",
    "        \n",
    "        self.mean_rating_per_item_df = (\n",
    "            ratings_df\n",
    "            .groupby('item')\n",
    "            .agg({'rating': 'mean'})\n",
    "        )\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5522\n",
      "Global Mean Baseline RMSE: 1.5521792341520126\n",
      "RMSE: 1.4388\n",
      "Popular Items Baseline RMSE: 1.4388169704620213\n"
     ]
    }
   ],
   "source": [
    "# Create baseline models\n",
    "global_mean_baseline = GlobalMeanBaseline()\n",
    "popular_baseline = PopularBaseline()\n",
    "\n",
    "# Train the baseline models\n",
    "global_mean_baseline.fit(trainset)\n",
    "popular_baseline.fit(trainset)\n",
    "\n",
    "# Make predictions\n",
    "global_predictions = global_mean_baseline.test(testset)\n",
    "popular_predictions = popular_baseline.test(testset)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "print(\"Global Mean Baseline RMSE:\", accuracy.rmse(global_predictions))\n",
    "print(\"Popular Items Baseline RMSE:\", accuracy.rmse(popular_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create base models\n",
    "def create_base_models(random_state=my_seed):\n",
    "    models = {\n",
    "        # Model-based CF approaches\n",
    "        'SVD': SVD(random_state=random_state),\n",
    "        'SVDpp': SVDpp(random_state=random_state),\n",
    "        'NMF': NMF(random_state=random_state),\n",
    "        \n",
    "        # Memory-based CF approaches\n",
    "        'KNN_Basic': KNNBasic(sim_options={'name': 'cosine', 'user_based': False}),\n",
    "        'KNN_Means': KNNWithMeans(sim_options={'name': 'pearson', 'user_based': False}),\n",
    "        'KNN_ZScore': KNNWithZScore(sim_options={'name': 'pearson', 'user_based': False}),\n",
    "        \n",
    "        # User-based approaches\n",
    "        'User_KNN_Basic': KNNBasic(sim_options={'name': 'cosine', 'user_based': True}),\n",
    "        'User_KNN_Means': KNNWithMeans(sim_options={'name': 'pearson', 'user_based': True})\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# Create base models\n",
    "base_models = create_base_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base models and collecting predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7d4d2c8d241c1963ab7ab71f87d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Models:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1970\n",
      "SVD: RMSE = 1.1970, Train Time = 2.64s, Predict Time = 0.53s\n",
      "RMSE: 1.1986\n",
      "SVDpp: RMSE = 1.1986, Train Time = 237.10s, Predict Time = 18.94s\n",
      "RMSE: 2.2201\n",
      "NMF: RMSE = 2.2201, Train Time = 5.66s, Predict Time = 0.55s\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.3654\n",
      "KNN_Basic: RMSE = 1.3654, Train Time = 5.20s, Predict Time = 12.91s\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.2301\n",
      "KNN_Means: RMSE = 1.2301, Train Time = 6.72s, Predict Time = 13.78s\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.2309\n",
      "KNN_ZScore: RMSE = 1.2309, Train Time = 7.01s, Predict Time = 13.85s\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.4359\n",
      "User_KNN_Basic: RMSE = 1.4359, Train Time = 3.67s, Predict Time = 12.26s\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.2218\n",
      "User_KNN_Means: RMSE = 1.2218, Train Time = 4.67s, Predict Time = 12.47s\n"
     ]
    }
   ],
   "source": [
    "# Function to train models and get predictions\n",
    "def train_and_predict(models, trainset, testset):\n",
    "    all_predictions = {}\n",
    "    all_models = {}\n",
    "    \n",
    "    for name, model in tqdm(models.items(), desc=\"Training Models\"):\n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        model.fit(trainset)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        start_time = time.time()\n",
    "        predictions = model.test(testset)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Store predictions and trained model\n",
    "        all_predictions[name] = predictions\n",
    "        all_models[name] = model\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = accuracy.rmse(predictions)\n",
    "        print(f\"{name}: RMSE = {rmse:.4f}, Train Time = {train_time:.2f}s, Predict Time = {predict_time:.2f}s\")\n",
    "    \n",
    "    return all_predictions, all_models\n",
    "\n",
    "# Train models and get predictions\n",
    "print(\"Training base models and collecting predictions...\")\n",
    "all_predictions, all_models = train_and_predict(base_models, trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating meta-dataset for stacking...\n",
      "Meta-dataset shape: (64602, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>actual</th>\n",
       "      <th>SVD</th>\n",
       "      <th>SVDpp</th>\n",
       "      <th>NMF</th>\n",
       "      <th>KNN_Basic</th>\n",
       "      <th>KNN_Means</th>\n",
       "      <th>KNN_ZScore</th>\n",
       "      <th>User_KNN_Basic</th>\n",
       "      <th>User_KNN_Means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57662</td>\n",
       "      <td>225</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.663576</td>\n",
       "      <td>8.774269</td>\n",
       "      <td>6.364171</td>\n",
       "      <td>8.046684</td>\n",
       "      <td>7.635886</td>\n",
       "      <td>7.788609</td>\n",
       "      <td>7.425114</td>\n",
       "      <td>7.194170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25928</td>\n",
       "      <td>3455</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.578248</td>\n",
       "      <td>7.335597</td>\n",
       "      <td>5.863711</td>\n",
       "      <td>8.398509</td>\n",
       "      <td>7.902761</td>\n",
       "      <td>7.955644</td>\n",
       "      <td>7.200902</td>\n",
       "      <td>7.370286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14445</td>\n",
       "      <td>3702</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.470841</td>\n",
       "      <td>6.506500</td>\n",
       "      <td>4.641915</td>\n",
       "      <td>5.889604</td>\n",
       "      <td>5.854128</td>\n",
       "      <td>5.648326</td>\n",
       "      <td>8.750435</td>\n",
       "      <td>6.602672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11483</td>\n",
       "      <td>3091</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.747190</td>\n",
       "      <td>9.686937</td>\n",
       "      <td>8.146973</td>\n",
       "      <td>9.303771</td>\n",
       "      <td>9.821812</td>\n",
       "      <td>9.749918</td>\n",
       "      <td>8.674906</td>\n",
       "      <td>9.926645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21786</td>\n",
       "      <td>8525</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.174973</td>\n",
       "      <td>8.362307</td>\n",
       "      <td>6.476377</td>\n",
       "      <td>8.449149</td>\n",
       "      <td>8.663591</td>\n",
       "      <td>8.651908</td>\n",
       "      <td>8.850686</td>\n",
       "      <td>8.850351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user  item  actual       SVD     SVDpp       NMF  KNN_Basic  KNN_Means  \\\n",
       "0  57662   225    10.0  7.663576  8.774269  6.364171   8.046684   7.635886   \n",
       "1  25928  3455     6.0  7.578248  7.335597  5.863711   8.398509   7.902761   \n",
       "2  14445  3702     8.0  6.470841  6.506500  4.641915   5.889604   5.854128   \n",
       "3  11483  3091    10.0  9.747190  9.686937  8.146973   9.303771   9.821812   \n",
       "4  21786  8525     9.0  9.174973  8.362307  6.476377   8.449149   8.663591   \n",
       "\n",
       "   KNN_ZScore  User_KNN_Basic  User_KNN_Means  \n",
       "0    7.788609        7.425114        7.194170  \n",
       "1    7.955644        7.200902        7.370286  \n",
       "2    5.648326        8.750435        6.602672  \n",
       "3    9.749918        8.674906        9.926645  \n",
       "4    8.651908        8.850686        8.850351  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a meta-dataset for stacking\n",
    "def create_meta_dataset(all_predictions, testset):\n",
    "    # Create a dictionary to store all predictions\n",
    "    meta_data = {}\n",
    "    \n",
    "    # Organize predictions by user-item pair\n",
    "    for model_name, predictions in all_predictions.items():\n",
    "        for pred in predictions:\n",
    "            user = pred.uid\n",
    "            item = pred.iid\n",
    "            rating = pred.r_ui\n",
    "            estimated = pred.est\n",
    "            \n",
    "            # Create keys for each user-item pair\n",
    "            key = (user, item)\n",
    "            \n",
    "            if key not in meta_data:\n",
    "                # Ensure we only add data points that are actually in the testset\n",
    "                # This check might be redundant if all_predictions only contains testset predictions\n",
    "                # but it's a safeguard.\n",
    "                meta_data[key] = {\n",
    "                    'actual': rating,\n",
    "                    'predictions': {}\n",
    "                }\n",
    "            \n",
    "            # Add this model's prediction\n",
    "            meta_data[key]['predictions'][model_name] = estimated\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    # Iterate through the testset to ensure order and inclusion\n",
    "    testset_keys = set([(pred.uid, pred.iid) for pred in testset])\n",
    "    for key in meta_data:\n",
    "      if key in testset_keys:\n",
    "          data = meta_data[key]\n",
    "          row = {'user': key[0], 'item': key[1], 'actual': data['actual']}\n",
    "          # Ensure all models have a prediction for this key\n",
    "          all_models_present = True\n",
    "          for model_name in all_predictions.keys():\n",
    "              if model_name not in data['predictions']:\n",
    "                  # This case should ideally not happen if predictions were generated correctly\n",
    "                  # Handle missing prediction (e.g., use global mean or skip row)\n",
    "                  # For simplicity, we might skip or fill with a default value\n",
    "                  # print(f\"Warning: Missing prediction for {model_name} on {key}\") \n",
    "                  all_models_present = False\n",
    "                  break \n",
    "              row[model_name] = data['predictions'][model_name]\n",
    "          if all_models_present:\n",
    "             rows.append(row)\n",
    "    \n",
    "    meta_df = pd.DataFrame(rows)\n",
    "    # Reorder columns to have 'user', 'item', 'actual' first, then model predictions\n",
    "    cols = ['user', 'item', 'actual'] + list(all_predictions.keys())\n",
    "    meta_df = meta_df[cols]\n",
    "    return meta_df\n",
    "\n",
    "# Create meta dataset using the actual testset tuples\n",
    "print(\"Creating meta-dataset for stacking...\")\n",
    "# Extract the actual testset tuples (uid, iid, rating)\n",
    "actual_testset_tuples = [(uid, iid, r_ui) for uid, iid, r_ui in testset]\n",
    "# Pass the list of Prediction objects directly\n",
    "meta_df = create_meta_dataset(all_predictions, actual_testset_tuples)\n",
    "print(f\"Meta-dataset shape: {meta_df.shape}\")\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training meta-models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5842db2a903144fe9462ef7b7b66f06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Meta-Models:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_Average: RMSE = 1.2180\n",
      "Weighted_Average: RMSE = 1.2029\n",
      "Linear_Regression: RMSE = 1.1716\n",
      "Ridge_Regression: RMSE = 1.1716\n",
      "Random_Forest: RMSE = 1.1737\n",
      "Gradient_Boosting: RMSE = 1.1698\n"
     ]
    }
   ],
   "source": [
    "# Create training data for meta-model\n",
    "X = meta_df.drop(['user', 'item', 'actual'], axis=1)\n",
    "y = meta_df['actual']\n",
    "\n",
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=my_seed)\n",
    "\n",
    "# Define meta-models\n",
    "meta_models = {\n",
    "    'Simple_Average': None,  # We'll implement this differently\n",
    "    'Weighted_Average': None,  # We'll implement this differently\n",
    "    'Linear_Regression': LinearRegression(),\n",
    "    'Ridge_Regression': Ridge(alpha=1.0),\n",
    "    'Random_Forest': RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=10, random_state=my_seed,\n",
    "        n_jobs=-1 if not cuda_available else 1  # Use all CPU cores if GPU not available\n",
    "    ),\n",
    "    'Gradient_Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, random_state=my_seed\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate meta-models\n",
    "meta_results = {}\n",
    "\n",
    "print(\"Training meta-models...\")\n",
    "for name, model in tqdm(meta_models.items(), desc=\"Training Meta-Models\"):\n",
    "    if name == 'Simple_Average':\n",
    "        # Simple average prediction\n",
    "        preds = X_val.mean(axis=1)\n",
    "        meta_results[name] = {\n",
    "            'model': None,\n",
    "            'predictions': preds,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_val, preds))\n",
    "        }\n",
    "    \n",
    "    elif name == 'Weighted_Average':\n",
    "        # Weighted average based on individual model RMSE on the test set\n",
    "        weights = {}\n",
    "        total_inv_rmse = 0\n",
    "        for model_name in base_models.keys():\n",
    "            # Calculate RMSE on the full test set predictions stored earlier\n",
    "            rmse = accuracy.rmse(all_predictions[model_name], verbose=False) # Use stored predictions\n",
    "            # Inverse weighting - lower RMSE gets higher weight\n",
    "            # Add a small epsilon to avoid division by zero if RMSE is somehow 0\n",
    "            inv_rmse = 1 / (rmse + 1e-9)\n",
    "            weights[model_name] = inv_rmse\n",
    "            total_inv_rmse += inv_rmse\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = {k: v / total_inv_rmse for k, v in weights.items()}\n",
    "        \n",
    "        # Apply weights to the validation set predictions (X_val)\n",
    "        weighted_preds = np.zeros(len(X_val))\n",
    "        for model_name, weight in weights.items():\n",
    "            weighted_preds += X_val[model_name].values * weight\n",
    "        \n",
    "        meta_results[name] = {\n",
    "            'model': weights, # Store the calculated weights\n",
    "            'predictions': weighted_preds,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_val, weighted_preds))\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_val)\n",
    "        \n",
    "        # Store results\n",
    "        meta_results[name] = {\n",
    "            'model': model,\n",
    "            'predictions': preds,\n",
    "            'rmse': np.sqrt(mean_squared_error(y_val, preds))\n",
    "        }\n",
    "    \n",
    "    print(f\"{name}: RMSE = {meta_results[name]['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building GPU-accelerated ensemble...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78656a9ab3e640e9beacf04432d3e3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training NN Ensemble:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: Loss = 1.8355, RMSE = 1.3548\n",
      "Epoch 20/50: Loss = 1.4302, RMSE = 1.1959\n",
      "Epoch 30/50: Loss = 1.5565, RMSE = 1.2476\n",
      "Epoch 40/50: Loss = 1.4169, RMSE = 1.1903\n",
      "Epoch 50/50: Loss = 1.4109, RMSE = 1.1878\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    print(\"Building GPU-accelerated ensemble...\")\n",
    "    \n",
    "    # Define a simple neural network meta-model\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "    class EnsembleNet(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(EnsembleNet, self).__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(input_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = EnsembleNet(input_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    epochs = 50\n",
    "    best_val_rmse = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training NN Ensemble\"):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            \n",
    "            # Convert back to CPU for RMSE calculation\n",
    "            val_preds = val_outputs.cpu().numpy().flatten()\n",
    "            val_actual = y_val_tensor.cpu().numpy().flatten()\n",
    "            val_rmse = np.sqrt(mean_squared_error(val_actual, val_preds))\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}: Loss = {val_loss:.4f}, RMSE = {val_rmse:.4f}\")\n",
    "            \n",
    "            # Save the best model based on validation RMSE\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best NN model with validation RMSE: {best_val_rmse:.4f}\")\n",
    "        # Re-evaluate with the best model to get final validation predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            final_val_outputs = model(X_val_tensor)\n",
    "            final_val_preds = final_val_outputs.cpu().numpy().flatten()\n",
    "            final_val_rmse = np.sqrt(mean_squared_error(y_val_tensor.cpu().numpy().flatten(), final_val_preds))\n",
    "    else:\n",
    "        # Fallback if training didn't improve\n",
    "        final_val_preds = val_preds\n",
    "        final_val_rmse = val_rmse\n",
    "\n",
    "    # Add NN results to meta_results\n",
    "    meta_results['Neural_Network'] = {\n",
    "        'model': model, # Store the trained model instance (best state loaded)\n",
    "        'predictions': final_val_preds, # Predictions from the best model on validation set\n",
    "        'rmse': final_val_rmse # Best validation RMSE achieved\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best meta-model: Gradient_Boosting with RMSE = 1.1698\n",
      "Generating final predictions using Gradient_Boosting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8fffe1782c42e9ad853f8a272c3ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Final Predictions:   0%|          | 0/64602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1478\n",
      "Final Ensemble RMSE: 1.1478\n"
     ]
    }
   ],
   "source": [
    "# Add this line before the function definition\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn.utils.validation')\n",
    "\n",
    "# Function to generate final predictions\n",
    "def generate_final_predictions(testset, base_models, meta_model_name, meta_models_dict, meta_results):\n",
    "    print(f\"Generating final predictions using {meta_model_name}...\")\n",
    "    \n",
    "    # Get the selected meta-model object or weights\n",
    "    meta_model_info = meta_results[meta_model_name]\n",
    "    meta_model = meta_model_info['model'] # This is the trained sklearn model, NN model, or weights dict\n",
    "    \n",
    "    # Store final predictions\n",
    "    final_predictions = []\n",
    "    \n",
    "    # Get feature names (order matters for sklearn models and NN)\n",
    "    feature_names = list(base_models.keys())\n",
    "    \n",
    "    for uid, iid, r_ui in tqdm(testset, desc=\"Generating Final Predictions\"):\n",
    "        # Collect predictions from all base models for this user-item pair\n",
    "        base_preds = {}\n",
    "        for name, model in base_models.items():\n",
    "            # Use the stored, already trained base models\n",
    "            pred = model.predict(uid, iid)\n",
    "            base_preds[name] = pred.est\n",
    "        \n",
    "        # Prepare input for the meta-model in the correct order\n",
    "        input_features = [base_preds[name] for name in feature_names]\n",
    "\n",
    "        # Make final prediction based on meta-model type\n",
    "        if meta_model_name == 'Simple_Average':\n",
    "            final_est = sum(input_features) / len(input_features)\n",
    "        elif meta_model_name == 'Weighted_Average':\n",
    "            # meta_model here is the dictionary of weights\n",
    "            final_est = sum(base_preds[name] * weight for name, weight in meta_model.items())\n",
    "        elif meta_model_name == 'Neural_Network':\n",
    "            # meta_model here is the trained PyTorch model\n",
    "            meta_model.eval() # Ensure model is in evaluation mode\n",
    "            X_test = torch.tensor([input_features], dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                final_est = meta_model(X_test).cpu().numpy()[0][0]\n",
    "        else: # Sklearn meta-models (LinearRegression, Ridge, RF, GB)\n",
    "            # meta_model here is the trained sklearn model\n",
    "            # Create DataFrame for prediction, ensuring column order matches training\n",
    "            X_test = pd.DataFrame([input_features], columns=feature_names)\n",
    "            final_est = meta_model.predict(X_test)[0]\n",
    "        \n",
    "        # Ensure prediction is within rating scale\n",
    "        final_est = max(min(final_est, max_rating), min_rating)\n",
    "        \n",
    "        # Create surprise.Prediction object\n",
    "        # Use the actual rating (r_ui) from the testset tuple\n",
    "        final_predictions.append(Prediction(uid, iid, r_ui, final_est, {}))\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "# Select best meta-model based on validation RMSE\n",
    "best_meta_model_name = min(meta_results, key=lambda k: meta_results[k]['rmse'])\n",
    "print(f\"Best meta-model: {best_meta_model_name} with RMSE = {meta_results[best_meta_model_name]['rmse']:.4f}\")\n",
    "\n",
    "# Generate final predictions using the best meta-model on the original testset\n",
    "# 'all_models' contains the trained base models\n",
    "# 'meta_models' defined earlier contains the sklearn instances (but we need the trained ones from meta_results)\n",
    "# 'meta_results' contains the trained models and weights\n",
    "final_predictions = generate_final_predictions(\n",
    "    actual_testset_tuples, # Use the original testset tuples (uid, iid, r_ui)\n",
    "    all_models,            # Dict of trained base models\n",
    "    best_meta_model_name,  # Name of the best meta-model\n",
    "    meta_models,           # Original dict defining meta-model types (less relevant here)\n",
    "    meta_results           # Dict containing trained meta-models/weights\n",
    ")\n",
    "\n",
    "# Calculate final RMSE on the test set\n",
    "final_rmse = accuracy.rmse(final_predictions)\n",
    "print(f\"Final Ensemble RMSE: {final_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1970\n",
      "RMSE: 1.1986\n",
      "RMSE: 2.2201\n",
      "RMSE: 1.3654\n",
      "RMSE: 1.2301\n",
      "RMSE: 1.2309\n",
      "RMSE: 1.4359\n",
      "RMSE: 1.2218\n",
      "RMSE: 1.1478\n"
     ]
    },
    {
     "data": {
      "image/png": "",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare performance of individual models and ensemble\n",
    "def plot_model_comparison(all_predictions, final_predictions):\n",
    "    model_rmse = {}\n",
    "    for name, predictions in all_predictions.items():\n",
    "        model_rmse[name] = accuracy.rmse(predictions, verbose=False)\n",
    "    \n",
    "    # Add ensemble\n",
    "    model_rmse['Ensemble (' + best_meta_model_name + ')'] = accuracy.rmse(final_predictions, verbose=False)\n",
    "    \n",
    "    # Sort by RMSE\n",
    "    model_rmse = {k: v for k, v in sorted(model_rmse.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_rmse.keys(), model_rmse.values())\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('RMSE (lower is better)')\n",
    "    plt.title('Performance Comparison of Base Models vs Best Ensemble')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot model comparison\n",
    "plot_model_comparison(all_predictions, final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained models...\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained models for future use\n",
    "def save_models(base_models_dict, meta_model_name, meta_results_dict):\n",
    "    print(\"Saving trained models...\")\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    \n",
    "    # Save base models (already trained, stored in all_models)\n",
    "    for name, model in base_models_dict.items():\n",
    "        with open(f'models/{name}_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    # Save the best meta model\n",
    "    best_meta_model_info = meta_results_dict[meta_model_name]\n",
    "    meta_model_obj = best_meta_model_info['model']\n",
    "\n",
    "    if meta_model_name == 'Simple_Average':\n",
    "        print(\"Simple_Average is computed on the fly, not saved as a file.\")\n",
    "    elif meta_model_name == 'Weighted_Average':\n",
    "        # Save the weights dictionary\n",
    "        with open(f'models/meta_{meta_model_name}_weights.pkl', 'wb') as f:\n",
    "            pickle.dump(meta_model_obj, f) # meta_model_obj is the weights dict\n",
    "    elif meta_model_name == 'Neural_Network':\n",
    "        # For PyTorch model, save state dict\n",
    "        torch.save(meta_model_obj.state_dict(), f'models/meta_{meta_model_name}_model.pt') # meta_model_obj is the NN model\n",
    "    else: # Sklearn models\n",
    "        with open(f'models/meta_{meta_model_name}_model.pkl', 'wb') as f:\n",
    "            pickle.dump(meta_model_obj, f) # meta_model_obj is the sklearn model\n",
    "    \n",
    "    print(\"Models saved successfully!\")\n",
    "\n",
    "# Save models (using the trained base models and the best meta model)\n",
    "save_models(all_models, best_meta_model_name, meta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'final_predictions' variable generated in cell 14 already stores \n",
    "# the ensemble model's predictions in the surprise.Prediction format.\n",
    "# This format is crucial for compatibility with evaluation tools, \n",
    "# particularly for calculating non-accuracy metrics like coverage, \n",
    "# personalization, and diversity using libraries such as recmetrics.\n",
    "# The base models (SVD, KNN, etc.) also produce predictions in this \n",
    "# format when their .test() method is called, as seen in cell 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "REC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
